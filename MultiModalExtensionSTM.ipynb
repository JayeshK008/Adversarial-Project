{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c28b7c-9f43-45f3-9f74-f5f3bbcfe4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from styleaug import StyleAugmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a462596-f1a6-4742-8c08-49c376ab09f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf250dcf0114626af20ae0c02f8d329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "streamed_dataset = load_dataset(\"jxie/coco_captions\", split=\"validation\", streaming=True)\n",
    "count = 0\n",
    "for example in streamed_dataset:\n",
    "    count += 1\n",
    "    if count >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad886891-f715-42ee-a411-699c66ff7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_by_tensor(t, t_min, t_max):\n",
    "    result = (t >= t_min).float() * t + (t < t_min).float() * t_min\n",
    "    result = (result <= t_max).float() * result + (result > t_max).float() * t_max\n",
    "    return result\n",
    "\n",
    "def save_image(images, names, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for img, name in zip(images, names):\n",
    "        img = Image.fromarray(img.astype('uint8'))\n",
    "        img.save(os.path.join(output_dir, f\"{name}.png\"))\n",
    "\n",
    "def gkern(kernlen=7, nsig=3):\n",
    "    import scipy.stats as st\n",
    "    x = np.linspace(-nsig, nsig, kernlen)\n",
    "    kern1d = st.norm.pdf(x)\n",
    "    kernel_raw = np.outer(kern1d, kern1d)\n",
    "    kernel = kernel_raw / kernel_raw.sum()\n",
    "    kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "    return kernel\n",
    "\n",
    "T_kernel = gkern(7, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4c86c-1d22-453a-a245-de280f548293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clip_images(images):\n",
    "    \"\"\"\n",
    "    Preprocess images to match CLIP's expected input format.\n",
    "    :param images: Tensor of shape [batch_size, 3, H, W], values in [0, 1]\n",
    "    :return: Tensor of shape [batch_size, 3, 224, 224], normalized\n",
    "    \"\"\"\n",
    "\n",
    "    resize = transforms.Resize((224, 224))\n",
    "    images = resize(images)\n",
    "\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(images.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(images.device).view(1, 3, 1, 1)\n",
    "    images = (images - mean) / std\n",
    "    return images\n",
    "\n",
    "def STM(images, captions, model, processor, min_val, max_val, device):\n",
    "    Resize = transforms.Resize(size=(224, 224))\n",
    "    momentum = 1.0\n",
    "    num_iter = 10\n",
    "    eps = 16.0 / 255.0\n",
    "    alpha = eps / num_iter\n",
    "    x = images.clone().to(device)\n",
    "    grad = torch.zeros_like(x).to(device)\n",
    "    N = 20\n",
    "    beta = 2.0\n",
    "    gamma = 0.5\n",
    "    augmentor = StyleAugmentor()\n",
    "\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_outputs = model.get_text_features(**text_inputs)\n",
    "        text_outputs = text_outputs / text_outputs.norm(dim=-1, keepdim=True)  # Normalize\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        noise = torch.zeros_like(x).to(device)\n",
    "        for n in range(N):\n",
    "            x_aug = augmentor(x).to(device)\n",
    "            x_new = gamma * x + (1 - gamma) * Resize(x_aug.detach()) + torch.randn_like(x).uniform_(-eps * beta, eps * beta)\n",
    "            x_new = V(x_new, requires_grad=True).to(device)\n",
    "\n",
    "            image_inputs = preprocess_clip_images(x_new)\n",
    "\n",
    "            image_outputs = model.get_image_features(pixel_values=image_inputs)\n",
    "            image_outputs = image_outputs / image_outputs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits_per_image = torch.matmul(image_outputs, text_outputs.t()) * model.logit_scale.exp()\n",
    "\n",
    "            batch_size = x_new.size(0)\n",
    "            labels = torch.arange(batch_size).to(device)\n",
    "            loss = F.cross_entropy(logits_per_image, labels)\n",
    "\n",
    "\n",
    "            noise += torch.autograd.grad(loss, x_new, retain_graph=False, create_graph=False)[0]\n",
    "        noise = noise / N\n",
    "\n",
    "        noise = noise / torch.abs(noise).mean([1, 2, 3], keepdim=True)\n",
    "        noise = momentum * grad + noise\n",
    "        grad = noise\n",
    "\n",
    "        x = x + alpha * torch.sign(noise)\n",
    "        x = clip_by_tensor(x, min_val, max_val)\n",
    "    return x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863e32ca-c809-45f6-b5ba-de3b49e6e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoStreamDataset(IterableDataset):\n",
    "    def __init__(self, hf_streamed_dataset, max_samples=None):\n",
    "        self.dataset = hf_streamed_dataset\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        for example in self.dataset:\n",
    "            image = example[\"image\"].convert(\"RGB\")\n",
    "            caption = example[\"caption\"]\n",
    "            cocoid = example[\"cocoid\"]\n",
    "            yield {\"image\": image, \"caption\": caption, \"cocoid\": cocoid}\n",
    "            count += 1\n",
    "            if self.max_samples is not None and count >= self.max_samples:\n",
    "                break\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    captions = [item[\"caption\"] for item in batch]\n",
    "    cocoids = [item[\"cocoid\"] for item in batch]\n",
    "    return {\"image\": images, \"caption\": captions, \"cocoid\": cocoids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379fff1-7b34-429f-b9f0-655dcb73767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec1d21-8518-43a6-a96c-ac7816f42548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [14:20, 53.81s/it]\n"
     ]
    }
   ],
   "source": [
    "T_kernel = T_kernel.to(device)\n",
    "dataset = CocoStreamDataset(streamed_dataset, max_samples=500)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=custom_collate_fn)\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "output_dir = \"clip_stm_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "  images = batch[\"image\"]\n",
    "  captions = batch[\"caption\"]\n",
    "  cocoids = batch[\"cocoid\"]\n",
    "\n",
    "  images_tensor = torch.stack([image_transform(img) for img in images]).to(device)\n",
    "\n",
    "  images_min = clip_by_tensor(images_tensor - 16.0 / 255.0, 0.0, 1.0)\n",
    "  images_max = clip_by_tensor(images_tensor + 16.0 / 255.0, 0.0, 1.0)\n",
    "\n",
    "  adv_images = STM(images_tensor, captions, model, processor, images_min, images_max, device)\n",
    "\n",
    "  adv_img_np = adv_images.cpu().numpy()\n",
    "  adv_img_np = np.transpose(adv_img_np, (0, 2, 3, 1)) * 255\n",
    "  save_image(adv_img_np, cocoids, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854cb179-bb67-4020-b0dc-42e5b8838a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adversarial_image(cocoid, adv_dir):\n",
    "    adv_path = os.path.join(adv_dir, f\"{cocoid}.png\")\n",
    "    if os.path.exists(adv_path):\n",
    "        return Image.open(adv_path).convert(\"RGB\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Adversarial image {adv_path} not found\")\n",
    "\n",
    "def evaluate_adversarial_images():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model.eval()\n",
    "\n",
    "    dataset = CocoStreamDataset(streamed_dataset, max_samples=500)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, collate_fn=custom_collate_fn)\n",
    "\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    adv_dir = \"clip_stm_outputs\"\n",
    "    similarities_orig = []\n",
    "    similarities_adv = []\n",
    "    linf_norms = []\n",
    "    top1_orig = 0\n",
    "    top1_adv = 0\n",
    "    top5_orig = 0\n",
    "    top5_adv = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        orig_images = batch[\"image\"]\n",
    "        captions = batch[\"caption\"]\n",
    "        cocoids = batch[\"cocoid\"]\n",
    "\n",
    "        try:\n",
    "            adv_images = [load_adversarial_image(cocoid, adv_dir) for cocoid in cocoids]\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        inputs_orig = processor(text=captions, images=orig_images, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        inputs_adv = processor(text=captions, images=adv_images, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_orig = model(**inputs_orig)\n",
    "            logits_per_image_orig = outputs_orig.logits_per_image  \n",
    "            similarity_orig = logits_per_image_orig.diag() / model.logit_scale.exp()  \n",
    "\n",
    "            outputs_adv = model(**inputs_adv)\n",
    "            logits_per_image_adv = outputs_adv.logits_per_image\n",
    "            similarity_adv = logits_per_image_adv.diag() / model.logit_scale.exp()\n",
    "\n",
    "        similarities_orig.extend(similarity_orig.cpu().numpy())\n",
    "        similarities_adv.extend(similarity_adv.cpu().numpy())\n",
    "        batch_size = logits_per_image_orig.size(0)\n",
    "\n",
    "\n",
    "        total += batch_size\n",
    "\n",
    "    avg_similarity_orig = np.mean(similarities_orig)\n",
    "    avg_similarity_adv = np.mean(similarities_adv)\n",
    "    \n",
    "\n",
    "    asr = np.mean(np.array(similarities_adv) < np.array(similarities_orig) - 0.1)  \n",
    "\n",
    "    print(f\"Evaluation Results (over {total} images):\")\n",
    "    print(f\"Average Cosine Similarity (Original): {avg_similarity_orig:.4f}\")\n",
    "    print(f\"Average Cosine Similarity (Adversarial): {avg_similarity_adv:.4f}\")\n",
    "    print(f\"Similarity Drop: {avg_similarity_orig - avg_similarity_adv:.4f}\")\n",
    "    print(f\"Attack Success Rate (ASR, margin=0.1): {asr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13a059d-ea86-45b6-8cc0-36f79eb8c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:13,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results (over 500 images):\n",
      "Average Cosine Similarity (Original): 0.3099\n",
      "Average Cosine Similarity (Adversarial): 0.2047\n",
      "Similarity Drop: 0.1052\n",
      "Attack Success Rate (ASR, margin=0.1): 0.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_adversarial_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b785b",
   "metadata": {},
   "source": [
    "## Novelty: STM Attack Evaluation for Multimodal Models\n",
    "\n",
    "**Evaluation Results (over 500 images):**\n",
    "- **Average Cosine Similarity (Original)**: 0.3099\n",
    "- **Average Cosine Similarity (Adversarial)**: 0.2047\n",
    "- **Similarity Drop**: 0.1052\n",
    "- **Attack Success Rate (ASR, margin=0.1)**: 0.5260\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8088db-a071-4f38-ad06-d129a28a7249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
